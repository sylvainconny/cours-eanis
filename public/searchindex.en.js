var relearn_searchindex = [
  {
    "breadcrumb": "Accueil",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Bases de la vidéo",
    "uri": "/cours-eanis/bases-video/index.html"
  },
  {
    "breadcrumb": "Accueil \u003e Bases de la vidéo",
    "content": "C’est le nombre de pixels dans une image. S’écrit avec un rapport largeur x hauteur. Exemple HD: 1920 x 1080 = 2 073 600 pixels Il y a 1920 colonnes et 1080 lignes de pixels.\nElle peut être appelée résolution, mais c’est un anglicisme. La résolution, en français, représente la densité de pixels, plus exactement c’est le nombre de pixels par unité de longueur (en général le pouce = 2.54cm), exprimée en dpi (Dot Per Inch) ou ppp (Point / Pixel Par Pouce). Un écran de smartphone FHD a une plus grande résolution qu’un téléviseur FHD.\nIl existe plusieurs normes :\nNormes de diffusion TV SD : 640 x 480 HD : 1280 x 720 Full HD : 1920 x 1080 Ultra HD : 3840 x 2160 Anciennes normes SD :\nPour NTSC : 720 x 480 Pour PAL/SECAM : 720 x 576 Normes Cinéma DCI (Digital Cinema Initiative) :\n2K : 2048 x 1080 4K : 4096 x 2160 Autres normes professionnelles :\n8K : 8192 x 4320 La règle de calcul est `xK = 1024x | 540x\" Autres normes :\nQuad HD : 2560 x 1440 À noter que si l’on a des images UHD, il peut être pertinent de les intégrer dans un projet FHD pour pouvoir zoomer dans l’image.",
    "description": "C’est le nombre de pixels dans une image. S’écrit avec un rapport largeur x hauteur.",
    "tags": [],
    "title": "Définition Image",
    "uri": "/cours-eanis/bases-video/definition-image/index.html"
  },
  {
    "breadcrumb": "Accueil \u003e Couleurs",
    "content": "Un espace colorimétrique c’est une zone définie dans le diagramme de chromaticité, lequel représente l’ensemble des chrominances (ou couleurs pures sous-entendu sans luminance, sans les nuances de luminosité) visibles à l’œil nu.\nL’espace Lab* représente en 3D la chrominance par la luminance, donc le réel espace complet des couleurs visibles à l’œil nu. On parle aussi de Gamut pour la chrominance (soit l’ensemble des couleurs que l’on peut créer avec du vert pur, du bleu pur et du rouge pur) et de Gamma pour la luminance (soit les nuances de gris).\nSi un espace colorimétrique couvre une partie de gamut, il est généralement associé à un gamma parmi lesquels 2.2, 2.4, 2.6, 2.8.\nEspaces de diffusion Dans l’ordre d’apparition et de grandeur :\nRec 601, le plus ancien, définit pour des normes SD Rec 709, définit initialement pour des normes HD et Full HD, même s’il est encore utilisé pour l’UHD Rec 2020, pour l’UHD Rec 2100, pensé pour le 8K Ces espaces colorimétriques représentent des gammes de couleurs disponibles différentes pour la capture, l’encodage et la diffusion, même si tous les écrans ne restituent pas forcément un espace à 100%.\nAu cinéma, c’est le DCI P3 qui est utilisé. Il rentre dans la norme des fichiers DCP pour la projection cinéma. Concernant le gamma, le Rec 709 étant généralement associé au 2.4 tandis que le cinéma plutôt au 2.6, cela peut dépendre du lieu de diffusion.\nDans le monde de la photo ou du web il existe également AdobeRGB et sRGB. Espaces de traitement Dépend du constructeur de caméra, on parle souvent de Log mais c’est un abus de langage car le log n’est en fait qu’un gamma, associé à un gamut de traitement.\nLe Log donne aux images cet aspect laiteux, mais c’est une conséquence de la largeur de son gamut et son gamma.\nParmi les plus courants :\nSony : S-log 2 et S-log 3 Canon : C-log Panasonic : V-log DJI : D-Log Arri : log C, log C3, log C4 RED : REDlogFilm Blackmagic : BlackmagicFilm Apple : Apple Log 2 Ici ce qui change d’un log à l’autre, c’est bien le gamma, car chaque constructeur est également associé à un gamut de traitement propre, comme le ARRI Wide Gamut : LUT Les Lookup Table ou tables de correspondances servent à passer d’un espace colorimétrique à un autre, comme de S-log 2 à Rec 709 par exemple. Il y a deux types de LUT :\nles LUT 1D, qui ne modifient que le gamma, servant essentiellement à corriger des erreurs d’enregistrement (exemple : mauvais gamut sur un espace de traitement) les LUT 3D qui modifient le gamma et le gamut, qui sont les LUT les plus généralement utilisées pour passer d’un espace à l’autre. L’utilité principale est pour l’étalonneur de pouvoir observer l’image dans l’espace de diffusion tout en conservant la grande quantité d’information des espaces de traitement. Un étalonneur de cinéma va utiliser une LUT log vers DCI P3, pour la télévision vers Rec 709. Ce qu’on connait mieux, ce sont les LUT artistiques qui servent à avoir facilement un rendu flatteur.\nACES / DaVinci Wide Gamut Ce sont des espaces colorimétriques “potentiels” allant au-delà de ce que l’oeil humain peut voir. Ils servent d’espace de travail entre l’espace d’origine et l’espace de sortie, et ne pose ainsi aucun problème de compatibilité entre les espaces colorimétriques des différentes sources.\nNetflix utilise également l’ACES pour automatiser l’espace de diffusion en fonction des capacités de l’écran de visionnage.",
    "description": "Un espace colorimétrique c’est une zone définie dans le diagramme de chromaticité, lequel représente l’ensemble des chrominances (ou couleurs pures sous-entendu sans luminance, sans les nuances de luminosité) visibles à l’œil nu",
    "tags": [],
    "title": "Espaces Colorimétriques",
    "uri": "/cours-eanis/couleurs/espaces-colorimetriques/index.html"
  },
  {
    "breadcrumb": "Accueil \u003e Bases de la vidéo",
    "content": "L’aspect ratio c’est le rapport entre la largeur et la hauteur de l’image.\n16/9 (ou 16:9), le plus connu qui correspond aux normes de définition TV 4/3 1:1 À noter que 16/9 = 1,77 donc on peut aussi l’écrire 1,77:1\nCette norme d’écriture facilite la visualisation :\n1920 | 16 | 1,77 _______________________ | | | | | | 1080 | 9 | 1 | | | | _______________________ Egalement, le 4/3 c’est du 1,33:1\nExemple, si on a des rushes en Full HD qu’on souhaite exporter dans un ratio 2:1, on se retrouvera avec une définition de 1920x960 car on préserve la largeur (ici 2) et pour la hauteur on divise la largeur par 2, donc 960.\nSi on souhaite maintenant un ratio cinémascope 2,4:1 on divise la largeur par 2,4 et ça donne du 1920x800.\nÀ l’inverse, si à partir d’une définition on souhaite retrouver le ratio d’une définition, par exemple 1920x768, le ratio étant le rapport largeur / hauteur on effectue l’opération suivante :\n1920 / 768 = 2.5 Le ratio est donc de 2.5:1 ou simplement 2.5\nIl existe des sites pour calculer facilement ces opérations comme unravel.\nQuelques exemples de définitions vs ratios :\n720x480 (SD NTSC) | 1.5:1 800x800 | 1:1 Carré 1080x1920 (Full HD vertical) | 9/16 0.56:1 1280x720 (HD) | 1.77:1 16/9 1440x1080 | 1.33:1 (4/3) 1920x640 | 3:1 1920x800 | 2.4:1 1920x1080 (Full HD) | 1.77:1 (16/9) 1920x1536 | 1.25:1 1920x2560 | 0.75:1 (3/4) 2000x1000 | 2:1 2160x3840 (UHD vertical) | 9/16 3840x1536 | 2.5:1 3840x2160 (UHD) | 1.77:1 (16/9) 4096x2160 (4K) | 1.89:1 (mal nommé 17/9)",
    "description": "L’aspect ratio  c’est le rapport entre la largeur et la hauteur de l’image",
    "tags": [],
    "title": "Aspect Ratio",
    "uri": "/cours-eanis/bases-video/aspect-ratio/index.html"
  },
  {
    "breadcrumb": "Accueil",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Couleurs",
    "uri": "/cours-eanis/couleurs/index.html"
  },
  {
    "breadcrumb": "Accueil \u003e Couleurs",
    "content": "Chroma subsampling en anglais. Désigne la compression de la chrominance, du plus ou moins compressé :\n4:2:0, pour la diffusion, beaucoup compressé 4:2:2, pour le traitement, peu compressé 4:4:4, aussi pour le traitement, pas de compression L’échantillonnage ne s’applique que sur les couleurs, la chrominance.\nLe premier chiffre désigne le nombre de pixels traités, 4 indique deux lignes de 4 pixels. Le deuxième et le troisième chiffre indiquent respectivement le niveau de compression sur la première et la deuxième ligne.\nDans le 4:4:4, les pixels sont laissés tels quels. Dans le 4:2:2, on ne conserve que la moitié des pixels de chaque ligne, en faisant la moyenne des deux premiers pixels et la moyenne des deux suivants. Dans le 4:2:0, on fait pareil pour la première ligne que dans le 4:2:2, mais la deuxième ligne elle sera remplacée par les pixels de la première.\nSi on n’évite de filmer en 4:2:0, c’est pour des problèmes d’aliasing ou de cross color. Par exemple, des rayures sur les vêtements peuvent être mal capturées, n’étant plus droites ou ajoutant des couleurs qui n’existent pas. Tourner du fond vert dans cet échantillonnage par exemple peut être problématique, les cheveux par exemple se retrouveraient mélangés avec le vert du fond.\nDans le codec Apple ProRes 4444, le dernier 4 indique la gestion d’une couche alpha, un masque du noir au blanc, où noir c’est non visible alors que blanc c’est visible.",
    "description": "Désigne la compression de la chrominance, du plus ou moins compressé",
    "tags": [],
    "title": "Échantillonage",
    "uri": "/cours-eanis/couleurs/echantillonage/index.html"
  },
  {
    "breadcrumb": "Accueil \u003e Bases de la vidéo",
    "content": "C’est le nombre d’images par seconde, présente deux unités :\nFPS (Frames Per Second) IPS (Images Par Seconde) Il existe trois grandes normes de diffusion :\n24fps est la norme cinéma qu’on retrouve dans les DCP, issue de l ‘apparition du son imposant ce minimum pour ne pas saccader le son lu en même temps que la pellicule 25fps est une norme télévisuelle héritée du PAL 29,97fps est une normale télévisuelle héritée du NTSC Ces deux dernières fréquences viennent des différences de courant électrique, en 50Hz pour les pays concernés par le PAL/SECAM, et 60Hz pour les pays ayant adopté le NTSC. Cette idée persiste notamment avec les ampoules, toujours en 50 ou 60Hz où le flickering apparaît.\nIl existe bien d’autres framerates relativement courants comme 23.976, 48, 50, 59.94, 60, 120…\nCes différents framerates permettent par exemple de faire des ralentis : une vidéo 50ips dans un projet en 25 sera ralentie par deux.\nL’interpolation (retime) d’image, c’est le fait de recréer des images manquantes. Dans un projet 25fps avec des sources 25fps, si on veut ralentir par deux un rush, il manquera des images et la vidéo sera saccadée. Le logiciel doit compléter ces lacunes :\nen interpolation par défaut, le logiciel recopie les images avec le frameblend, en fusionnant deux images avec une opacité à 50% ou l’optical flow (flux optique) où le logiciel calcule la différence entre deux images, mais fonctionne surtout pour les mouvements lents Ça peut dépanner mais il vaut mieux prévoir dans son projet le framerate final et donc un framerate de captation plus élevé pour faire des ralentis.\nVotre navigateur ne supporte pas la vidéo WebM. À noter également qu’un tournage à haut framerate, si l’on souhaite garder un flou de mouvement naturel on doit augmenter la fréquence d’obturation (ex : 50fps =\u003e 1/100), ce qui implique moins de lumière en entrée pour chaque image, donc une vidéo plus sombre, qu’il faut compenser.\nDans un projet en 25ips où l’on souhaite intégrer un timelapse, il serait nécessaire de prendre assez de clichés pour en avoir 25 par secondes.",
    "description": "C’est le nombre d’images par seconde",
    "tags": [],
    "title": "Framerate",
    "uri": "/cours-eanis/bases-video/framerate/index.html"
  },
  {
    "breadcrumb": "Accueil \u003e Couleurs",
    "content": "En anglais, Grayscale quantization ou level quantization. Désigne la compression de la luminance du plus au moins compressés et donc le nombre de nuances de gris :\n8 bits (2^8 soit 256 nuances de gris) 10 bits (2^10 = 1024) 12 bits (2^12 = 4096) 14 bits (2^14 = 16 384) 16 bits (2^16 = 65 536) 32 bits (2^32 = 4 294 967 296) La diffusion se fait essentiellement en 8 bits, exception faite des Blu-Ray qui peuvent monter jusqu’à 10 bits. Les caméras captent de 8 à 14 bits, sauf la Sony Venice 2 qui peut aller jusqu’à 16 bits. Le 32 bits ne peut être créé que numériquement.\nAinsi, quand on fait la synthèse additive avec les vert, bleu et rouge purs, en 8 bits donc 256 nuances par couleur ça fait 256x256x256 = 16 millions de couleurs.\nDu coup, en 8 bits, ce vert pur s’écrit R0 V255 B0 #00FF00.\nPour éviter l’effet de banding, on peut monter la quantification, mais vu qu’on diffuse en 8 bits la plupart du temps, c’est difficile à éviter.\nÀ noter qu’en lumière / vidéo, les couleurs primaires sont rouge / vert / bleu tandis qu’en peinture c’est rouge / jaune / bleu.",
    "description": "Désigne la compression de la luminance du plus au moins compressés et donc le nombre de nuances de gris",
    "tags": [],
    "title": "Quantification",
    "uri": "/cours-eanis/couleurs/quantification/index.html"
  },
  {
    "breadcrumb": "Accueil \u003e Bases de la vidéo",
    "content": "Consiste en l’adaptation d’une image à son support de diffusion, il existe 3 manière de faire :\nletterbox : “dézoome” l’image jusqu’à ce qu’elle rentre entièrement, quitte à laisser des “bandes noires”, du vide squeeze : remplis l’ensemble de l’écran avec l’image, quitte à la déformer edge crop : “zoome” l’image jusqu’à ce qu’elle remplisse entièrement l’écran, quitte à perdre de l’information",
    "description": "Consiste en l’adaptation d’une image à son support de diffusion",
    "tags": [],
    "title": "Anamorphose",
    "uri": "/cours-eanis/bases-video/anamorphose/index.html"
  },
  {
    "breadcrumb": "Accueil \u003e Bases de la vidéo",
    "content": "Il existe deux types d’affichage :\nprogressif implique que les images complètent s’affichent l’une après l’autre (1080p) entrelacé (interlaced) décompose l’image en lignes paires et impaires qui forment la trame paire et la trame impaire et les affiche l’une après l’autre (1080i) Le deuxième date des anciennes télévisions qui affichaient les images ligne par ligne et cet affichage une ligne sur deux permettaient d’éviter de voir l’image se créer sous nos yeux.\nDu coup, la dénomination change, une vidéo 1080p25 (Full HD Progresif à 25fps) devient pour la même vidéo en entrelacée 1080i50 (Full HD Entrelacé à 50 trames par secondes).\nC’est ce qui donne l’effet “peigne” aux anciennes vidéos lues dans un projet en progressif ou un export en progressif. Pour régler le problème, il faut désentrelacer la vidéo.",
    "description": "Progressif ou entrelacé ?",
    "tags": [],
    "title": "Affichage",
    "uri": "/cours-eanis/bases-video/affichage/index.html"
  },
  {
    "breadcrumb": "Accueil \u003e Bases de la vidéo",
    "content": "Littéralement “codeur / décodeur”, permet de compresser les fichiers vidéo afin de les rendre plus légers, et de les décompresser pour les rendre lisibles.\nIl existe plusieurs types de codecs :\nles codecs de diffusion permettant d’obtenir le fichier vidéo le plus léger possible tout en conservant la meilleure qualité visuelle possible les codecs de montage dont la priorité absolue est de conserver le plus d’information possible sans se préoccuper de la taille du fichier Codecs de diffusion MPEG-2 (norme et codec), le plus ancien, toujours utilisé par certaines chaînes de télévision H264 (ou AVC - Advanced Video Coding) fait partie de la norme MPEG-4 (différent de MP4, qui est un conteneur, voir plus bas) H265 (ou HEVC - High Efficiency Video Coding) qui à qualité visuelle également génèrera un fichier vidéo plus léger, il est surtout utilisé pour l’enregistrement dans des petits dispositifs qui capturent en 4K/UHD comme des drones ou des caméras d’action VP8/VP9, les codecs de Google sont plus performants que les H264 et H265, utilisé notamment par YouTube et qu’il est recommandé d’utiliser pour la publication sur la plateforme AV1, le plus performant actuellement, il est open source mais encore peu répandu même si très apprécié des plateformes de streaming D’autres codecs sont à venir comme le H266 (ou VVC - Versatile Video Coding) et le AV2.\nPour les métiers des VFX ou de l’étalonnage, il est nécessaire d’avoir les fichiers avec le plus d’informations possibles. Ce n’est pas recommandé non plus en montage, car plus un fichier est compressé, plus le décodage à la volée demandera des ressources à l’ordinateur, et une quantité importante de vidéos compressées dans un logiciel de montage peut rendre l’expérience plus difficile. De plus, ces codecs utilisent une méthode de compréhension nommée Long GOP (Group Of Pictures) dont le principe est de compresser via des groupes d’images selon trois types :\nles I Frame (Intra Frame) où le codec va enregistrer toute l’image, elle est peu ou pas compressée, elle sert de référence les P Frame (Predictive Frame) et B Frame (Bidirectional predictive Frame) qui ne garderont les différences avec l’image de référence (I Frame) C’est pour cette raison, à chaque fois qu’on fait un arrêt sur une frame précise, qu’on a une chance sur X (dépend du codec) de tomber sur une P ou B Frame que l’ordinateur doit recalculer à partir de son image de référence, ce qui n’est absolument pas optimal.\nÀ noter qu’il existe d’autres méthodes de compression comme l’INTRA qui ne contient que des I Frame, moins efficace que le Long GOP, plus utilisé dans les codecs de montage.\nCodecs de montage Il en existe deux grandes familles. D’abord, dans la famille Apple, créés avec l’arrivée de Final Cut, dont les plus courants sont, du plus au moins compressif et donc du plus léger au plus lourd :\nApple ProRes Proxy Apple ProRes 422 Apple ProRes 422HQ Apple ProRes 4444 Apple ProRes 4444 XQ Ces différents codecs existent aussi pour répondre aux besoins différents métiers, les monteurs nécessitent moins d’informations que les étalonneurs ou les VFX.\nBien que le transcodage permette par exemple de changer la méthode de compression d’un codec de diffusion (H264) à un codec de montage (Apple ProRes Proxy), donc de Long GOP à INTRA, ce qui doit fluidifier le montage car il n’y a plus que des I Frame, cela ne peut pas pour autant recréer les informations qui ont été perdues à l’encodage.\nL’autre famille c’est DNx du constructeur AVID avec deux sous-familles le DNxHD, plus ancienne avec des codecs permettant d’aller jusqu’au Full HD dans des framerates très normés, et le DNxHR pour tout le reste.\nDans l’ordre du plus au moins compressif :\nDNxHD LB (anciennement DNxHD 36) DNxHD SQ (anciennement DNxHD 120) DNxHD HQ (anciennement DNxHD 185) DNxHD HQx (anciennement DNxHD 185x) et DNxHR LB DNxHR SQ DNxHR HQ DNxHR HQx DNxHR 444 À cas d’usage similaire, un codec équivalent de chaque famille est quasiment identique à quelques subtilités près.\nÀ noter que le x de HQx indique qu’il peut aller jusqu’à 10 bits de profondeur de pixel.\nCodecs “d’enregistrement” Ces codecs issus essentiellement des fabricants de caméras compressent extrêmement peu. Ils peuvent en sortir régulièrement, et tous les logiciels ne sont pas compatibles avec tous ces codecs, il peut être utile de se renseigner auprès du constructeur afin de connaître son utilisation dans son workflow. Quand bien même ils possèdent souvent la dénomination RAW, c’est un abus de langage car le simple fait que ce soit un codec n’en fait pas du RAW.\nDans les plus courants, on a :\nBlackmagic RAW REDcode RAW Sony X-OCN ArriRAW Apple ProRes RAW Bien qu’étant peu compressés, ces codecs ne sont pas adaptés au montage car l’immense quantité de données des fichiers en question rend la lecture très compliquée dû à la vitesse trop limitée des solutions de stockage (HDD, SDD). C’est pourquoi on utilise des proxys parmi les codecs de montage.\nBitrate / Débit vidéo Définit comme la quantité d’information par seconde de vidéo, notée en bits/seconde. Les codecs de montage n’étant pas concerné par la question du poids, on s’intéressera surtout au bitrate pour les codecs de diffusion.\nPar exemple, pour un fichier 1080p25 encodé en H264, la meilleure qualité visuelle atteignable se trouve pour un bitrate de 15Mbits/s. C’est une valeur de référence connue. Dans ce cas, il est donc inutile d’aller plus haut et à l’inverse, on peut donc réduire le bitrate pour alléger le fichier, aux dépends de la qualité, sachant qu’on garde une image magnifique entre 10 et 15Mbits/s.\nLogiquement pour une vidéo H264 en UHD à 25ips aura une qualité maximale à 60Mb/s (4 fois plus de pixels), 30Mb/s pour une vidéo Full HD à 50ips (2 fois plus d’images). Pour les autres codecs, notamment plus performants comme VP8/VP9 ou AV1, forcément à qualité égale le bitrate sera plus faible, mais connaître cette valeur “maximale” est à chercher.\nCBR / VBR Ce sont deux traitements de débit vidéo différents :\nCBR : Constant BitRate VBR : Variable BitRate Dans le premier cas c’est simple, chaque seconde de vidéo aura un bitrate fixe, par exemple 15Mb/s. Dans le deuxième, on choisit une fourchette, entre 10 et 15Mb/s par exemple, alors le codec donnera plus de bitrate pour les scènes compliquées à encoder, et moins pour les scènes plus simples. Ainsi, en VBR, la qualité sera équivalente au CBR mais le poids du fichier sera sensiblement plus léger, plus optimisé.\nLes conteneurs Différent du codec, improprement appelés extension, c’est le grand tiroir dans lequel on range tous les flux (audio / vidéo / sous-titres…) du fichier vidéo.\nQuelques exemples connus :\nMOV (Apple QuickTime) MP4 (MPEG4) AVI MXF (AVID) MKV Matroska WebM Les conteneurs sont relativement indépendants des codecs, un MOV peut contenir du H264 comme du ProRes, le MP4 peut contenir du H264 également.\nFichiers RAW Les fichiers RAW contiennent des images directement issues du capteur sans codec. Mais du coup, elles ne sont pas lisibles, l’intérêt étant de pouvoir choisir le codec, et un espace colorimétrique, on appelle ça la débayerisation.\nBien que la dénomination RAW sur les codecs d’enregistrement, ils permettent tout de même d’avoir plus la main sur le fichier et de pouvoir appliquer des traitements supplémentaires.",
    "description": "Littéralement ‘codeur / décodeur’, permet de compresser les fichiers vidéo afin de les rendre plus légers, et de les décompresser pour les rendre lisibles",
    "tags": [],
    "title": "Codecs",
    "uri": "/cours-eanis/bases-video/codecs/index.html"
  },
  {
    "breadcrumb": "",
    "content": "Sommaire Bases de la vidéo Définition image Aspect Ratio Framerate Anamorphose Affichage Codecs Couleurs Espaces colorimétriques Échantillonage Quantification Workflow recommandé Pour chaque projet, créer une arborescence de ce genre :\n. ├── PROJETS_EANIS │ └── NOM_PROJET │ └── RUSHES │ ├── PROXY │ ├── AUDIO │ └── VIDEO │ └── SONS │ ├── SFX │ └── MUSIQUES │ └── ELEMENTS │ ├── TITRES │ ├── MOTION │ └── STOCK │ └── PROJETS │ ├── PREMIERE │ └── AFTER │ ├── yyyymmdd_nom_projet.aep │ └── yyyymmdd_nom_projet.aep │ └── EXPORTS │ ├── FINAL │ ├── RENDUS_3D │ └── AFTER │ └── Resolve Project Library",
    "description": "Sommaire Bases de la vidéo Définition image Aspect Ratio Framerate Anamorphose Affichage Codecs Couleurs Espaces colorimétriques Échantillonage Quantification Workflow recommandé Pour chaque projet, créer une arborescence de ce genre :\n. ├── PROJETS_EANIS │ └── NOM_PROJET │ └── RUSHES │ ├── PROXY │ ├── AUDIO │ └── VIDEO │ └── SONS │ ├── SFX │ └── MUSIQUES │ └── ELEMENTS │ ├── TITRES │ ├── MOTION │ └── STOCK │ └── PROJETS │ ├── PREMIERE │ └── AFTER │ ├── yyyymmdd_nom_projet.aep │ └── yyyymmdd_nom_projet.aep │ └── EXPORTS │ ├── FINAL │ ├── RENDUS_3D │ └── AFTER │ └── Resolve Project Library",
    "tags": [],
    "title": "Accueil",
    "uri": "/cours-eanis/index.html"
  },
  {
    "breadcrumb": "Accueil",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/cours-eanis/categories/index.html"
  },
  {
    "breadcrumb": "Accueil",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/cours-eanis/tags/index.html"
  }
]
